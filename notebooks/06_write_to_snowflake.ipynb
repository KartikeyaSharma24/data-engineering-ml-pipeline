{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f35105-9970-4993-852e-392c7f072e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"snowflake-snowpark-python[pandas]\"\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbdb407-7f91-45f3-b187-da91be5b302e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data ready: (497472, 8) (1014, 5)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "# Read Spark tables\n",
    "silver_pdf = spark.table(\"workspace.default.stocks_silver\").toPandas()\n",
    "forecast_pdf = spark.table(\"workspace.default.stocks_forecast\").toPandas()\n",
    "\n",
    "# Rename columns to match Snowflake schema\n",
    "silver_pdf2 = silver_pdf.rename(columns={\n",
    "    \"symbol\":\"SYMBOL\",\"date\":\"DT\",\"close\":\"CLOSE\",\"volume\":\"VOLUME\",\"return\":\"RET_PCT\"\n",
    "})\n",
    "forecast_pdf2 = forecast_pdf.rename(columns={\n",
    "    \"symbol\":\"SYMBOL\",\"ds\":\"DS\",\"yhat\":\"YHAT\",\"yhat_lower\":\"YHAT_LOWER\",\"yhat_upper\":\"YHAT_UPPER\"\n",
    "})\n",
    "\n",
    "# Ensure date columns are proper dates\n",
    "silver_pdf2[\"DT\"] = pd.to_datetime(silver_pdf2[\"DT\"]).dt.date\n",
    "forecast_pdf2[\"DS\"] = pd.to_datetime(forecast_pdf2[\"DS\"]).dt.date\n",
    "\n",
    "print(\"✅ Data ready:\", silver_pdf2.shape, forecast_pdf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f578bee0-ac69-4a43-8da4-78c3fd7e270d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, math, tempfile, shutil\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "\n",
    "def put_and_copy(df: pd.DataFrame, table: str, cols: list[str], chunk_rows=100_000):\n",
    "    \"\"\"\n",
    "    Writes df to CSV chunks (columns forced to `cols` order),\n",
    "    PUTs to @%table, then COPY INTO table.\n",
    "    \"\"\"\n",
    "    # --- Force column order up-front & normalize dates if present ---\n",
    "    df = df.loc[:, cols].copy()\n",
    "    if \"DT\" in df.columns:\n",
    "        df[\"DT\"] = pd.to_datetime(df[\"DT\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "    if \"DS\" in df.columns:\n",
    "        df[\"DS\"] = pd.to_datetime(df[\"DS\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=f\"sf_load_{table}_\")\n",
    "    try:\n",
    "        total = len(df)\n",
    "        nchunks = max(1, math.ceil(total / chunk_rows))\n",
    "        print(f\"\uD83D\uDCE6 {table}: {total} rows → {nchunks} chunk(s)\")\n",
    "\n",
    "        # Write CSV chunks with header\n",
    "        filepaths = []\n",
    "        for i in range(nchunks):\n",
    "            start = i * chunk_rows\n",
    "            end   = min(start + chunk_rows, total)\n",
    "            chunk = df.iloc[start:end].copy()\n",
    "            # ⬇️ Force column order again on each chunk\n",
    "            chunk = chunk.loc[:, cols]\n",
    "            path  = os.path.join(tmpdir, f\"{table.lower()}_{i:04d}.csv\")\n",
    "            chunk.to_csv(path, index=False)\n",
    "            filepaths.append(path)\n",
    "        print(f\"\uD83D\uDDC2️  {table}: wrote {len(filepaths)} CSV file(s). First file: {os.path.basename(filepaths[0])}\")\n",
    "\n",
    "        # Connect to Snowflake\n",
    "        conn = snowflake.connector.connect(\n",
    "            user=SF_USER, password=SF_PASSWORD, account=SF_ACCOUNT,\n",
    "            warehouse=SF_WAREHOUSE, database=SF_DATABASE, schema=SF_SCHEMA, role=SF_ROLE,\n",
    "            login_timeout=60, client_session_keep_alive=True\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"USE WAREHOUSE {SF_WAREHOUSE}\")\n",
    "        cur.execute(f\"USE DATABASE {SF_DATABASE}\")\n",
    "        cur.execute(f\"USE SCHEMA {SF_SCHEMA}\")\n",
    "\n",
    "        # Clean old staged files for this table (optional safety)\n",
    "        cur.execute(f\"REMOVE @%{table} pattern='.*.csv.gz'\")\n",
    "\n",
    "        # PUT local CSVs → table stage (auto-compress to .gz)\n",
    "        for path in filepaths:\n",
    "            cur.execute(f\"PUT file://{path} @%{table} AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "        # COPY INTO table: now positions match exactly, thanks to enforced order\n",
    "        cols_sql = \",\".join(cols)\n",
    "        copy_sql = f\"\"\"\n",
    "            COPY INTO {table} ({cols_sql})\n",
    "            FROM @%{table}\n",
    "            FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\\\"' SKIP_HEADER=1 COMPRESSION=GZIP)\n",
    "            ON_ERROR='ABORT_STATEMENT'\n",
    "        \"\"\"\n",
    "        cur.execute(copy_sql)\n",
    "\n",
    "        # Verify count\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        cnt = cur.fetchone()[0]\n",
    "        print(f\"✅ Loaded {table}: total rows = {cnt}\")\n",
    "\n",
    "        # Clean stage files (optional)\n",
    "        cur.execute(f\"REMOVE @%{table} pattern='.*.csv.gz'\")\n",
    "        cur.close(); conn.close()\n",
    "    finally:\n",
    "        shutil.rmtree(tmpdir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5483f4-b48b-4fbc-a2a6-e933f78265fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 STOCK_FORECAST: 1014 rows → 1 chunk(s)\n\uD83D\uDDC2️  STOCK_FORECAST: wrote 1 CSV file(s). First file: stock_forecast_0000.csv\n✅ Loaded STOCK_FORECAST: total rows = 1014\n"
     ]
    }
   ],
   "source": [
    "# Forecast load first (1 chunk in your case)\n",
    "put_and_copy(\n",
    "    df   = forecast_pdf2,\n",
    "    table= \"STOCK_FORECAST\",\n",
    "    cols = [\"SYMBOL\",\"DS\",\"YHAT\",\"YHAT_LOWER\",\"YHAT_UPPER\"],\n",
    "    chunk_rows = 50_000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37fa90d-1059-4a94-8517-28361cb40ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 STOCK_SILVER: 497472 rows → 5 chunk(s)\n\uD83D\uDDC2️  STOCK_SILVER: wrote 5 CSV file(s). First file: stock_silver_0000.csv\n✅ Loaded STOCK_SILVER: total rows = 994944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "put_and_copy(\n",
    "    df   = silver_pdf2,\n",
    "    table= \"STOCK_SILVER\",\n",
    "    cols = [\"SYMBOL\",\"DT\",\"CLOSE\",\"VOLUME\",\"RET_PCT\"],\n",
    "    chunk_rows = 100_000\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_write_to_snowflake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}